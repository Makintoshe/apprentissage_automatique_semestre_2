{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 sur la regression logistique multi-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisé par : MULAPI TITA Ketsia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP est d’implementer un coût softmax, qui sert à étendre la régression logistique pour\n",
    "la classification multiclasse. Dans ce contexte, la modélisation de la probabilité d’appartenance à\n",
    "une classe est donnée par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où C est le nombre de classes, et wk est un vecteur de R^d. On appelle cette fonction, la fonction softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(y = λ_k|x_i) $ est modélisé par la focntion softam ci-dessus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous allez implémenter dans ce TP, un modèle linéaire multiclasse appris tel que :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En considérant que yi,j représente la probabilité d’appartenance à une classe λj de l’exemple xi\n",
    ", la\n",
    "fonction de coût cross-entropy s’écrit :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où yˆi,j = p(y = λk|xi) est modélisé par la fonction softmax ci-dessus. Vous allez implémenter dans ce TP, un modèle linéaire multiclasse appris tel que"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où W représente une matrice de taille d × C contenant l’ensemble des {w_k}\n",
    "\n",
    "> Commencez par charger les données relatives à un problème de reconnaissance de chiffres\n",
    "manuscrits :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "n_class = 10\n",
    "X,y = load_digits(n_class=n_class , return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc25917a6d8>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "plt.imshow(X[4].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez les fonctions permettant :\n",
    "    \n",
    "    ◦ de transformer les étiquettes des données en vecteurs de probabilités d’appartenance aux\n",
    "    classes, à l’aide d’un encodage one-hot 2\n",
    "    . Bien entendu, pour les données d’apprentissage,\n",
    "    ces probabilités sont soit égales à 1 (pour la classe à laquelle appartient la donnée), soit\n",
    "    égale à 0 (pour les autres classes).\n",
    "    \n",
    "    ◦ d’évaluer une fonction softmax étant donné un z = x\n",
    "    >w\n",
    "    \n",
    "    ◦ d’estimer la probabilité d’appartenance d’un ensemble de données étant donné W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodage(y, n_class):\n",
    "    '''\n",
    "        Cette fonction renvoie une matrice N,n_class qui\n",
    "        représente pour chaque exemple l'appartenance à une classe sur n_class total.\n",
    "        0 : n'appartient pas à la classe\n",
    "        1 : appartien à la classe\n",
    "        n : nombre d'exemples\n",
    "        y : les vérités terrains\n",
    "        n_class ::: le nombre de classes\n",
    "    '''\n",
    "    n = y.shape[0]\n",
    "    y_one = np.zeros((n,n_class))\n",
    "    for i in range(n):\n",
    "        y_one[i,y[i]]=1\n",
    "    return y_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    ''' \n",
    "        cette fonction renvoie pour chaque exemples un ensemble de n_class \"probabilités\"\n",
    "        z ::: on sait que z = X^⁽T⁾@Wk un vecteur\n",
    "    '''\n",
    "    # computational trick for numerical stability\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T/ np.sum(np.exp(z),axis=1)).T\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_pred(X, W):\n",
    "    '''\n",
    "        Cette fonction récupère les probabilités d'appartenance d'un exemple pour chaque classe et,\n",
    "        permet de récupérer l'argument (la classe) à la plus grande probabilité\n",
    "        X ::: ensemble de données d'e\n",
    "        W ::: la matrice de taille s x C qui contien tl'ensemble des {Wk} \n",
    "    '''\n",
    "    probs = softmax(X@W) # \n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    print(\"probs : \",probs)\n",
    "    print(\"\\npreds : \",preds)\n",
    "    return probs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant une fonction qui calcule la fonction de coût et le gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_grad(W,X,y,lam ,n_class):\n",
    "    '''\n",
    "    Cette focntion renvoie une valeur loss et, \n",
    "    un gradient sous forme de matrice ayant la même taille que W\n",
    "    '''\n",
    "    # convert the integer class coding into a one-hot representation\n",
    "    y_mat = oneHotEncodage(y,n_class) \n",
    "    # compute raw class scores given our input and current weights\n",
    "    scores = X@W \n",
    "    # perform a softmax on these scores to get their probabilities\n",
    "    prob = softmax(scores)     \n",
    "    # insérer votre code ici\n",
    "    n = X.shape[0]\n",
    "    loss = (-1/n) * np.sum(y_mat * np.log(prob)) + (lam/2) * np.sum(W.T@W)\n",
    "    grad = (-1/n) * X.T @ (y_mat - prob) + lam * W # \n",
    "    return loss , grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Implémentez une descente de gradient sans backtracking pour optimiser les paramètres W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_without_backtracking(X, y, n_class, methode, lam=1):\n",
    "    '''\n",
    "    Cette focntion implémente la méthode de descente de gradient\n",
    "    avec un pas fixe !\n",
    "    elle renvoi alors un vecteur de loss et, la matrice W\n",
    "    '''\n",
    "    W = np.zeros([X.shape[1],len(np.unique(y))]) # o au départ\n",
    "    pas = 0.01\n",
    "    losses = []\n",
    "    iter_max = 1000\n",
    "    for i in range(iter_max):\n",
    "        loss, grad = methode(W,X,y,lam,n_class)\n",
    "        losses.append(loss)\n",
    "        W = W - pas*grad\n",
    "    print(grad.shape)\n",
    "    print(loss)\n",
    "    print(W.shape)\n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problème de classification des chiffres manuscits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluez l’erreur en classification sur les données d’apprentissage après entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 1 : apprentisssage (entraînement de l'algorithme, recherche des paramètres W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n",
      "0.27025718025443\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "W, losses_without_backtrack = steepest_descent_without_backtracking(X_train,y_train,n_class,get_loss_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 2 : On prédit ... ON obtient les probabilités et les predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs :  [[7.43108574e-03 6.37728182e-03 2.45532057e-03 ... 2.24247537e-03\n",
      "  1.53034033e-02 5.74827342e-03]\n",
      " [2.11082024e-02 3.54513871e-03 3.79963027e-03 ... 1.08462409e-02\n",
      "  1.78251579e-02 7.75982019e-01]\n",
      " [4.31581759e-04 2.08373195e-03 6.64007460e-03 ... 1.95010767e-03\n",
      "  1.75296048e-02 1.33591848e-02]\n",
      " ...\n",
      " [1.83170014e-02 2.84736986e-03 2.70287856e-04 ... 3.05911204e-03\n",
      "  4.14473026e-03 1.56023760e-04]\n",
      " [5.13595485e-03 1.97179495e-03 3.85293988e-03 ... 1.93908572e-03\n",
      "  4.18405884e-02 8.91737764e-01]\n",
      " [9.23748357e-01 3.61868969e-04 3.30557130e-02 ... 2.84150355e-03\n",
      "  1.03070041e-02 9.61864298e-03]]\n",
      "\n",
      "preds :  [6 9 3 7 2 2 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
      " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4\n",
      " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
      " 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
      " 0 3 5 6 6 0 6 4 3 9 3 8 7 2 9 0 4 5 8 6 5 7 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
      " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 1 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
      " 1 6 4 5 6 0 3 2 3 6 7 1 9 1 4 7 6 5 1 5 5 1 0 2 8 8 9 5 7 6 2 2 2 3 4 8 8\n",
      " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
      " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
      " 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 9 8 7 4 3 8 3 5 6 0 0 3 0 5 0 0 4 1\n",
      " 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4\n",
      " 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 2 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8\n",
      " 3 3 6 2 6 5 4 2 0 8 7 3 7 0 2 2 3 5 8 7 3 6 5 9 9 2 5 6 3 0 7 1 1 9 6 1 1\n",
      " 0 0 2 9 8 9 9 3 7 7 1 3 5 4 6 8 2 1 1 8 7 6 9 2 0 4 4 8 8 7 9 3 1 7 1 3 5\n",
      " 1 7 0 0 2 2 6 9 4 8 9 0 6 7 7 9 5 4 7 0 7 6 8 7 1 4 6 2 8 7 5 9 0 3 9 6 6\n",
      " 1 9 1 2 9 8 9 7 7 8 5 5 9 7 7 6 8 9 3 5 7 9 5 9 2 1 1 2 2 4 8 7 5 8 8 9 4\n",
      " 9 0]\n"
     ]
    }
   ],
   "source": [
    "prob, pred = get_prob_pred(X_test,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, 7, 2, 2, 5, 2, 5, 2, 1, 9, 4, 0, 4, 2, 3, 7, 8, 8, 4, 3,\n",
       "       9, 7, 5, 6, 3, 5, 6, 3, 4, 9, 1, 4, 4, 6, 9, 4, 7, 6, 6, 9, 1, 3,\n",
       "       6, 1, 3, 0, 6, 5, 5, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 5, 2, 4, 5,\n",
       "       7, 0, 7, 5, 9, 9, 5, 4, 7, 0, 4, 5, 5, 9, 9, 0, 2, 3, 8, 0, 6, 4,\n",
       "       4, 9, 1, 2, 8, 3, 5, 2, 9, 0, 4, 4, 4, 3, 5, 3, 1, 3, 5, 9, 4, 2,\n",
       "       7, 7, 4, 4, 1, 9, 2, 7, 8, 7, 2, 6, 9, 4, 0, 7, 2, 7, 5, 8, 7, 5,\n",
       "       7, 9, 0, 6, 6, 4, 2, 8, 0, 9, 4, 6, 9, 9, 6, 9, 0, 3, 5, 6, 6, 0,\n",
       "       6, 4, 3, 9, 3, 8, 7, 2, 9, 0, 4, 5, 8, 6, 5, 7, 9, 8, 4, 2, 1, 3,\n",
       "       7, 7, 2, 2, 3, 9, 8, 0, 3, 2, 2, 5, 6, 9, 9, 4, 1, 5, 4, 2, 3, 6,\n",
       "       4, 8, 5, 9, 5, 7, 1, 9, 4, 8, 1, 5, 4, 4, 9, 6, 1, 8, 6, 0, 4, 5,\n",
       "       2, 7, 1, 6, 4, 5, 6, 0, 3, 2, 3, 6, 7, 1, 9, 1, 4, 7, 6, 5, 1, 5,\n",
       "       5, 1, 0, 2, 8, 8, 9, 5, 7, 6, 2, 2, 2, 3, 4, 8, 8, 3, 6, 0, 9, 7,\n",
       "       7, 0, 1, 0, 4, 5, 1, 5, 3, 6, 0, 4, 1, 0, 0, 3, 6, 5, 9, 7, 3, 5,\n",
       "       5, 9, 9, 8, 5, 3, 3, 2, 0, 5, 8, 3, 4, 0, 2, 4, 6, 4, 3, 4, 5, 0,\n",
       "       5, 2, 1, 3, 1, 4, 1, 1, 7, 0, 1, 5, 2, 1, 2, 8, 7, 0, 6, 4, 8, 8,\n",
       "       5, 1, 8, 4, 5, 8, 7, 9, 8, 6, 0, 6, 2, 0, 7, 9, 8, 9, 5, 2, 7, 7,\n",
       "       9, 8, 7, 4, 3, 8, 3, 5, 6, 0, 0, 3, 0, 5, 0, 0, 4, 1, 2, 8, 4, 5,\n",
       "       9, 6, 3, 1, 8, 8, 4, 2, 3, 8, 9, 8, 8, 5, 0, 6, 3, 3, 7, 1, 6, 4,\n",
       "       1, 2, 1, 1, 6, 4, 7, 4, 8, 3, 4, 0, 5, 1, 9, 4, 5, 7, 6, 3, 7, 0,\n",
       "       5, 9, 7, 5, 9, 7, 4, 2, 2, 9, 0, 7, 5, 8, 3, 6, 3, 9, 6, 9, 5, 0,\n",
       "       1, 5, 5, 8, 3, 3, 6, 2, 6, 5, 4, 2, 0, 8, 7, 3, 7, 0, 2, 2, 3, 5,\n",
       "       8, 7, 3, 6, 5, 9, 9, 2, 5, 6, 3, 0, 7, 1, 1, 9, 6, 1, 1, 0, 0, 2,\n",
       "       9, 8, 9, 9, 3, 7, 7, 1, 3, 5, 4, 6, 8, 2, 1, 1, 8, 7, 6, 9, 2, 0,\n",
       "       4, 4, 8, 8, 7, 9, 3, 1, 7, 1, 3, 5, 1, 7, 0, 0, 2, 2, 6, 9, 4, 8,\n",
       "       9, 0, 6, 7, 7, 9, 5, 4, 7, 0, 7, 6, 8, 7, 1, 4, 6, 2, 8, 7, 5, 9,\n",
       "       0, 3, 9, 6, 6, 1, 9, 1, 2, 9, 8, 9, 7, 7, 8, 5, 5, 9, 7, 7, 6, 8,\n",
       "       9, 3, 5, 7, 9, 5, 9, 2, 1, 1, 2, 2, 4, 8, 7, 5, 8, 8, 9, 4, 9, 0])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 3 : On récupère les y_test sous la forme multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_one = oneHotEncodage(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut déjà voir que notre prediction est à peu près correcte car à 1 = 9 pour y_one et, 9 pour preds.\n",
    "\n",
    "pareille pour (n-1) et n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on vérifie qu'on a bien 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_one[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On évalue les performances de notre modèle : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_score(X, y, w, n_class):\n",
    "    '''\n",
    "    Cette fonction renvoi le taux d'erreur en classification et, \n",
    "    le taux de bonne classification.\n",
    "    elle attend :\n",
    "    X : le X à utiliser pour la prédiciton\n",
    "    y : la réalité\n",
    "    w : les paramètres du modèles après entraînement\n",
    "    n_class : le nombre de classe\n",
    "    '''\n",
    "    n = len(y)\n",
    "    prob, pred = get_prob_pred(X,w)\n",
    "    accuracy_score = sum(pred == y)/n\n",
    "    return 1-accuracy_score, accuracy_score, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord on vérifie avec les mêmes données utilisés pour l'entraînement, on sera content de voir que l'on s'est permit un minimum d'erreur et donc que l'on ne sur apprend pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs :  [[5.57713593e-04 8.71628026e-01 6.02530796e-04 ... 1.37384225e-03\n",
      "  2.81184420e-02 4.60222123e-02]\n",
      " [2.15031035e-04 8.35648338e-01 6.44560284e-02 ... 1.62789351e-02\n",
      "  4.75404402e-02 1.24644548e-03]\n",
      " [6.42719898e-03 1.80008621e-02 4.84755382e-02 ... 4.74092937e-02\n",
      "  8.06153851e-01 2.04954879e-02]\n",
      " ...\n",
      " [1.62130746e-03 7.60244660e-03 8.68765987e-01 ... 5.35399019e-02\n",
      "  2.73368334e-02 2.54314244e-03]\n",
      " [1.78476269e-02 5.61184310e-02 1.36148299e-02 ... 5.45031848e-01\n",
      "  5.81031630e-02 3.38014120e-02]\n",
      " [3.85294277e-03 7.43748462e-01 2.31409986e-02 ... 2.17099098e-02\n",
      "  9.88056483e-02 2.50423343e-03]]\n",
      "\n",
      "preds :  [1 1 8 ... 2 7 1]\n"
     ]
    }
   ],
   "source": [
    "tx_err_train, tx_train = eval_score(X_train, y_train, W,n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d'erreur en apprentissage est :  0.029093931837073983  soit  2.9093931837073983 % et donc, le taux de bonne classification des données entraînés est : 0.970906068162926  soit :  97.0906068162926\n"
     ]
    }
   ],
   "source": [
    "print(\"Le taux d'erreur en apprentissage est : \",tx_err_train, \" soit \", tx_err_train*100,'% et donc, le taux de bonne classification des données entraînés est :',tx_train, \" soit : \",tx_train*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs :  [[7.43108574e-03 6.37728182e-03 2.45532057e-03 ... 2.24247537e-03\n",
      "  1.53034033e-02 5.74827342e-03]\n",
      " [2.11082024e-02 3.54513871e-03 3.79963027e-03 ... 1.08462409e-02\n",
      "  1.78251579e-02 7.75982019e-01]\n",
      " [4.31581759e-04 2.08373195e-03 6.64007460e-03 ... 1.95010767e-03\n",
      "  1.75296048e-02 1.33591848e-02]\n",
      " ...\n",
      " [1.83170014e-02 2.84736986e-03 2.70287856e-04 ... 3.05911204e-03\n",
      "  4.14473026e-03 1.56023760e-04]\n",
      " [5.13595485e-03 1.97179495e-03 3.85293988e-03 ... 1.93908572e-03\n",
      "  4.18405884e-02 8.91737764e-01]\n",
      " [9.23748357e-01 3.61868969e-04 3.30557130e-02 ... 2.84150355e-03\n",
      "  1.03070041e-02 9.61864298e-03]]\n",
      "\n",
      "preds :  [6 9 3 7 2 2 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
      " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4\n",
      " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
      " 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
      " 0 3 5 6 6 0 6 4 3 9 3 8 7 2 9 0 4 5 8 6 5 7 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
      " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 1 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
      " 1 6 4 5 6 0 3 2 3 6 7 1 9 1 4 7 6 5 1 5 5 1 0 2 8 8 9 5 7 6 2 2 2 3 4 8 8\n",
      " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
      " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
      " 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 9 8 7 4 3 8 3 5 6 0 0 3 0 5 0 0 4 1\n",
      " 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4\n",
      " 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 2 9 0 7 5 8 3 6 3 9 6 9 5 0 1 5 5 8\n",
      " 3 3 6 2 6 5 4 2 0 8 7 3 7 0 2 2 3 5 8 7 3 6 5 9 9 2 5 6 3 0 7 1 1 9 6 1 1\n",
      " 0 0 2 9 8 9 9 3 7 7 1 3 5 4 6 8 2 1 1 8 7 6 9 2 0 4 4 8 8 7 9 3 1 7 1 3 5\n",
      " 1 7 0 0 2 2 6 9 4 8 9 0 6 7 7 9 5 4 7 0 7 6 8 7 1 4 6 2 8 7 5 9 0 3 9 6 6\n",
      " 1 9 1 2 9 8 9 7 7 8 5 5 9 7 7 6 8 9 3 5 7 9 5 9 2 1 1 2 2 4 8 7 5 8 8 9 4\n",
      " 9 0]\n"
     ]
    }
   ],
   "source": [
    "tx_err_test, tx_test = eval_score(X_test, y_test, W, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on évalue convenablement notre modèle avec des données qu'il n'a jamais vu : X_test ici on sera content d'avoir une très bonne performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d'erreur en apprentissage est :  0.045454545454545414  soit  4.545454545454541 % et donc, le taux de bonne classification des données entraînés est : 0.9545454545454546  soit :  95.45454545454545\n"
     ]
    }
   ],
   "source": [
    "print(\"Le taux d'erreur en apprentissage est : \",tx_err_test, \" soit \", tx_err_test*100,'% et donc, le taux de bonne classification des données entraînés est :',tx_test, \" soit : \",tx_test*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichez l’évolution de la fonction objective après chaque itération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc2591bd128>]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_without_backtrack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constat :\n",
    "\n",
    "On remarque que plus on iter plus notre fonction de perte décroit de façon assez intérressante et donc, cela nous rassure que avec le pas fixe de 0.01 que l'on s'est fixé, l'algorithme de descente de gradient converge correctement ce qui explique les bons taux de performances obtenus."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f415e978cbbcfe4ee76e0f0ae60463270fd6aaaf7ce8526631058b94c49e7ac"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
